---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<!-- {% if site.author.googlescholar %}
  <div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %} -->

{% include base_path %}

<!-- New style rendering if publication categories are defined -->
<!-- {% if site.publication_category %}
  {% for category in site.publication_category  %}
    {% assign title_shown = false %}
    {% for post in site.publications reversed %}
      {% if post.category != category[0] %}
        {% continue %}
      {% endif %}
      {% unless title_shown %}
        <h2>{{ category[1].title }}</h2><hr />
        {% assign title_shown = true %}
      {% endunless %}
      {% include archive-single.html %}
    {% endfor %}
  {% endfor %}
{% else %}
  {% for post in site.publications reversed %}
    {% include archive-single.html %}
  {% endfor %}
{% endif %} -->

<p></p>

# Peer-Reviewed International Conference Paper

-  <u>Kazuki Yamauchi</u>, Yusuke Ijima, and Yuki Saito<br>
**StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models**<br>
IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2024. (Accepted)<br>
\[[arXiv](https://arxiv.org/abs/2311.16509)\] \[[demo](https://ntt-hilab-gensp.github.io/icassp2024stylecap/)\]



# Preprint

- Wataru Nakata\*, <u>Kazuki Yamauchi</u>\*, Dong Yang, Hiroaki Hyodo, and Yuki Saito (\*Equal contribution)<br>
**UTDUSS: UTokyo-SaruLab System for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge**<br>
Technical Report for Interspeech2024 Speech Processing Using Discrete Speech Unit Challenge, 5 pages, Mar. 2024.<br>
**Ranked 1st in TTS (Acoustic+Vocoder) track** \[[link](https://huggingface.co/spaces/discrete-speech/interspeech2024_discrete_speech_tts_full)\]<br>
\[[arXiv](https://arxiv.org/abs/2403.13720)\] \[[code](https://huggingface.co/sarulab-speech/UTDUSS-Vocoder)\]



# Domestic Conferences -Non Reviewed- (Japanese)

- <u>山内 一輝</u>, 井島 勇祐, 齋藤 佑樹<br>
**StyleCap: 音声および言語の自己教師あり学習モデルに基づく音声の発話スタイルに関するキャプション生成**<br>
日本音響学会 2024年春季研究発表会 講演論文集, 3-2-14, pp. 843--846, 2024年3月.<br>
\[[pdf](/files/yamauchi24asjs_paper.pdf)\] \[[slide](/files/yamauchi24asjs_slide.pdf)\]


- <u>山内 一輝</u>, 齋藤 佑樹, 猿渡 洋<br>
**VQ-VAEに基づく解釈可能なアクセント潜在変数を用いた多方言音声合成**<br>
電子情報通信学会研究報告, SP2023-80, Vol. 123, No. 403, pp.220--225, 2024年3月.<br>
**2024年 SP研究会 学生ポスター賞** \[[link](https://www.ieice.org/iss/sp/jpn/special/sp-poster-prize.html)\]<br>
\[[pdf](/files/yamauchi24sp03_paper.pdf)\] \[[poster](/files/yamauchi24sp03_poster.pdf)\]


- 織田 悠希, <u>山内 一輝</u>, 齋藤 佑樹, 猿渡 洋<br>
**クラウドソーシングで収集した方言アクセントラベルに基づく End-to-End 日本語音声合成の方言適応**<br>
電子情報通信学会研究報告, Vol. 123, No. 403, 2024年3月.


- <u>山内 一輝</u>, 齋藤 佑樹, 猿渡 洋<br>
**アクセント潜在変数の予測と制御が可能なTTSモデルによる方言音声合成の検討**<br>
日本音響学会 2023年秋季研究発表会 講演論文集, 2-Q-30, pp. 1255--1256, 2023年9月.<br>
\[[pdf](/files/yamauchi23asja_paper.pdf)\] \[[poster](/files/yamauchi23asja_poster.pdf)\]



